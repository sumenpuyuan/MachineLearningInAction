{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 准备数据，从文本中构建向量 ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['take', 'flea', 'dalmation', 'cute', 'mr', 'problem', 'licks', 'please', 'my', 'I', 'steak', 'dog', 'grabage', 'food', 'him', 'worthless', 'is', 'love', 'to', 'buyiing', 'how', 'ate', 'help', 'posting', 'quit', 'has', 'so', 'park', 'not', 'maybe', 'stupid', 'stop']\n"
     ]
    }
   ],
   "source": [
    "#词到向量的转换函数\n",
    "def loadDataSet():\n",
    "    postingList=[\n",
    "        ['my','dog','has','flea','problem','help','please'],\n",
    "        ['maybe','not','take','him','to','dog','park','stupid'],\n",
    "        ['my','dalmation','is','so','cute','I','love','him'],\n",
    "        ['stop','posting','stupid','worthless','grabage'],\n",
    "        ['mr','licks','ate','my','steak','how','to','stop','him'],\n",
    "        ['quit','buyiing','worthless','dog','food','stupid']\n",
    "    ]\n",
    "    classVec=[0,1,0,1,0,1]#1表示侮辱性文字 0表示正常文字\n",
    "    return postingList,classVec\n",
    "#生成不重复的单词\n",
    "def createVocabList(dataSet):\n",
    "    vocabSet=set([])\n",
    "    #合并\n",
    "    for document in dataSet:\n",
    "        vocabSet=vocabSet | set(document)\n",
    "    return list(vocabSet)\n",
    "def setOfWords2Vec(vocabList,inputSet):\n",
    "    returnVec=[0]*len(vocabList)#生成全是0 长度为字典长度的字典\n",
    "    for word in inputSet:\n",
    "        if word in vocabList:\n",
    "            returnVec[vocabList.index(word)]=1\n",
    "        else:\n",
    "            print('the word %s is not in my vocabulary'%(word))\n",
    "    return returnVec\n",
    "listOPosts,listClasses=loadDataSet()\n",
    "myVocabList=createVocabList(listOPosts)\n",
    "print(myVocabList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "setOfWords2Vec(myVocabList,listOPosts[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练算法，从词向量到计算概率 ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import *\n",
    "#朴素贝叶斯分类训练函数\n",
    "def trainNB0(trainMatrix,trainCategory):\n",
    "    numTrainDocs=len(trainMatrix)\n",
    "    numWords=len(trainMatrix[0])\n",
    "    pAbusive=sum(trainCategory)/float(numTrainDocs)\n",
    "    p0Num=zeros(numWords)\n",
    "    p1Num=zeros(numWords)\n",
    "    p0Denom=0.0\n",
    "    p1Denom=0.0\n",
    "    for i in range(numTrainDocs):\n",
    "        if trainCategory[i]==1:\n",
    "            p1Num+=trainMatrix[i]\n",
    "            p1Denom+=sum(trainMatrix[i])\n",
    "        else:\n",
    "            p0Num+=trainMatrix[i]\n",
    "            p0Denom+=sum(trainMatrix[i])\n",
    "    p1Vect=p1Num/p1Denom\n",
    "    p0Vect=p0Num/p0Denom\n",
    "    return p0Vect,p1Vect,pAbusive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n",
      "[0.         0.04166667 0.04166667 0.04166667 0.04166667 0.04166667\n",
      " 0.04166667 0.04166667 0.125      0.04166667 0.04166667 0.04166667\n",
      " 0.         0.         0.08333333 0.         0.04166667 0.04166667\n",
      " 0.04166667 0.         0.04166667 0.04166667 0.04166667 0.\n",
      " 0.         0.04166667 0.04166667 0.         0.         0.\n",
      " 0.         0.04166667]\n",
      "[0.05263158 0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.10526316\n",
      " 0.05263158 0.05263158 0.05263158 0.10526316 0.         0.\n",
      " 0.05263158 0.05263158 0.         0.         0.         0.05263158\n",
      " 0.05263158 0.         0.         0.05263158 0.05263158 0.05263158\n",
      " 0.15789474 0.05263158]\n"
     ]
    }
   ],
   "source": [
    "trainMat=[]\n",
    "for postinDoc in listOPosts:\n",
    "    trainMat.append(setOfWords2Vec(myVocabList,postinDoc))\n",
    "p0V,p1V,pAb=trainNB0(trainMat,listClasses)\n",
    "print(pAb)\n",
    "print(p0V)\n",
    "print(p1V)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 测试算法：根据现实情况修改分类器 ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.极大似然估计，出现0抹去的情况，使用拉普拉斯修正 ###\n",
    "### 2.连续相乘导致下溢 ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def trainNB0(trainMatrix,trainCategory):\n",
    "    numTrainDocs=len(trainMatrix)\n",
    "    numWords=len(trainMatrix[0])\n",
    "    pAbusive=sum(trainCategory)/float(numTrainDocs)\n",
    "    #使用拉普拉斯修正\n",
    "    p0Num=ones(numWords)\n",
    "    p1Num=ones(numWords)\n",
    "    p0Denom=2.0\n",
    "    p1Denom=2.0\n",
    "    for i in range(numTrainDocs):\n",
    "        if trainCategory[i]==1:\n",
    "            p1Num+=trainMatrix[i]\n",
    "            p1Denom+=sum(trainMatrix[i])\n",
    "        else:\n",
    "            p0Num+=trainMatrix[i]\n",
    "            p0Denom+=sum(trainMatrix[i])\n",
    "    #使用对数防止下溢\n",
    "    p1Vect=log(p1Num/p1Denom)\n",
    "    p0Vect=log(p0Num/p0Denom)\n",
    "    return p0Vect,p1Vect,pAbusive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 朴素贝叶斯分类函数 ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['love', 'my', 'dalmation'] classifyed as :  0\n",
      "the word garbage is not in my vocabulary\n",
      "['stupid', 'garbage'] classified as :  1\n"
     ]
    }
   ],
   "source": [
    "def classifyNB(vec2Classify,p0Vec,p1Vec,pClass1):\n",
    "    p1=sum(vec2Classify*p1Vec)+log(pClass1)\n",
    "    p0=sum(vec2Classify*p0Vec)+log(1.0-pClass1)\n",
    "    if p1>p0:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "def testingNB():\n",
    "    listOPosts,listClasses=loadDataSet()\n",
    "    myVocabList=createVocabList(listOPosts)\n",
    "    trainMat=[]\n",
    "    for postinDoc in listOPosts:\n",
    "        trainMat.append(setOfWords2Vec(myVocabList,postinDoc))\n",
    "    p0V,p1V,pAb=trainNB0(trainMat,listClasses)\n",
    "    testEntry=['love','my','dalmation']\n",
    "    thisDoc=array(setOfWords2Vec(myVocabList,testEntry))\n",
    "    print(testEntry,'classifyed as : ',classifyNB(thisDoc,p0V,p1V,pAb))\n",
    "    testEntry=['stupid','garbage']\n",
    "    thisDoc=array(setOfWords2Vec(myVocabList,testEntry))\n",
    "    print(testEntry,'classified as : ',classifyNB(thisDoc,p0V,p1V,pAb))\n",
    "testingNB()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  朴素贝叶斯词袋模型 ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def bagOfWords2VecMN(vocabList,inputSet):\n",
    "    returnVec=[0]*len(vocabList)\n",
    "    for word in inputSet:\n",
    "        if word in vocabList:\n",
    "            returnVec[vocabList.index(word)]+=1\n",
    "    return returnVec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 示例：使用朴素贝叶斯过滤垃圾邮件 #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['this',\n",
       " 'book',\n",
       " 'is',\n",
       " 'the',\n",
       " 'best',\n",
       " 'book',\n",
       " 'on',\n",
       " 'python',\n",
       " 'or',\n",
       " 'M.L',\n",
       " 'i',\n",
       " 'have',\n",
       " 'ever',\n",
       " 'laid',\n",
       " 'eyes',\n",
       " 'upon']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mySent='this book is the best book on python or M.L i have ever laid eyes upon'\n",
    "mySent.split()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看到，切分效果不错，但是标点符号也被当成了词的一部分，可以用正则表示来切分句子，其中分隔符是除单词，数字外的任意字符串"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:3: FutureWarning: split() requires a non-empty pattern match.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['this',\n",
       " 'book',\n",
       " 'is',\n",
       " 'the',\n",
       " 'best',\n",
       " 'book',\n",
       " 'on',\n",
       " 'python',\n",
       " 'or',\n",
       " 'M',\n",
       " 'L',\n",
       " 'i',\n",
       " 'have',\n",
       " 'ever',\n",
       " 'laid',\n",
       " 'eyes',\n",
       " 'upon']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "regEx=re.compile('\\\\W*')\n",
    "listOfTokens=regEx.split(mySent)\n",
    "listOfTokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['this',\n",
       " 'book',\n",
       " 'is',\n",
       " 'the',\n",
       " 'best',\n",
       " 'book',\n",
       " 'on',\n",
       " 'python',\n",
       " 'or',\n",
       " 'm',\n",
       " 'l',\n",
       " 'i',\n",
       " 'have',\n",
       " 'ever',\n",
       " 'laid',\n",
       " 'eyes',\n",
       " 'upon']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[tok.lower() for tok in listOfTokens if(len(tok)>0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:2: FutureWarning: split() requires a non-empty pattern match.\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "emailText=open('email/ham/6.txt').read()\n",
    "listOfTokens=regEx.split(emailText)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 测试：使用朴素贝叶斯进行交叉验证 ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spamTest():\n",
    "    docList=[]; classList = []; fullText =[]\n",
    "    for i in range(1,26):\n",
    "        wordList = textParse(open('email/spam/%d.txt' % i).read())\n",
    "        docList.append(wordList)\n",
    "        fullText.extend(wordList)\n",
    "        classList.append(1)\n",
    "        wordList = textParse(open('email/ham/%d.txt' % i).read())\n",
    "        docList.append(wordList)\n",
    "        fullText.extend(wordList)\n",
    "        classList.append(0)\n",
    "    vocabList = createVocabList(docList)#create vocabulary\n",
    "    trainingSet = list(range(50)); testSet=[]           #create test set\n",
    "    for i in range(10):\n",
    "        randIndex = int(random.uniform(0,len(trainingSet)))\n",
    "        testSet.append(trainingSet[randIndex])\n",
    "        del(trainingSet[randIndex])  \n",
    "    trainMat=[]; trainClasses = []\n",
    "    for docIndex in trainingSet:#train the classifier (get probs) trainNB0\n",
    "        trainMat.append(bagOfWords2VecMN(vocabList, docList[docIndex]))\n",
    "        trainClasses.append(classList[docIndex])\n",
    "    p0V,p1V,pSpam = trainNB0(array(trainMat),array(trainClasses))\n",
    "    errorCount = 0\n",
    "    for docIndex in testSet:        #classify the remaining items\n",
    "        wordVector = bagOfWords2VecMN(vocabList, docList[docIndex])\n",
    "        if classifyNB(array(wordVector),p0V,p1V,pSpam) != classList[docIndex]:\n",
    "            errorCount += 1\n",
    "            print (\"classification error\",docList[docIndex])\n",
    "    print ('the error rate is: ',float(errorCount)/len(testSet))\n",
    "    #return vocabList,fullText\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classification error ['yay', 'you', 'both', 'doing', 'fine', 'working', 'mba', 'design', 'strategy', 'cca', 'top', 'art', 'school', 'new', 'program', 'focusing', 'more', 'right', 'brained', 'creative', 'and', 'strategic', 'approach', 'management', 'the', 'way', 'done', 'today']\n",
      "classification error ['home', 'based', 'business', 'opportunity', 'knocking', 'your', 'door', 'don抰', 'rude', 'and', 'let', 'this', 'chance', 'you', 'can', 'earn', 'great', 'income', 'and', 'find', 'your', 'financial', 'life', 'transformed', 'learn', 'more', 'here', 'your', 'success', 'work', 'from', 'home', 'finder', 'experts']\n",
      "classification error ['yeah', 'ready', 'may', 'not', 'here', 'because', 'jar', 'jar', 'has', 'plane', 'tickets', 'germany', 'for']\n",
      "classification error ['benoit', 'mandelbrot', '1924', '2010', 'benoit', 'mandelbrot', '1924', '2010', 'wilmott', 'team', 'benoit', 'mandelbrot', 'the', 'mathematician', 'the', 'father', 'fractal', 'mathematics', 'and', 'advocate', 'more', 'sophisticated', 'modelling', 'quantitative', 'finance', 'died', '14th', 'october', '2010', 'aged', 'wilmott', 'magazine', 'has', 'often', 'featured', 'mandelbrot', 'his', 'ideas', 'and', 'the', 'work', 'others', 'inspired', 'his', 'fundamental', 'insights', 'you', 'must', 'logged', 'view', 'these', 'articles', 'from', 'past', 'issues', 'wilmott', 'magazine']\n",
      "the error rate is:  0.4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda3\\lib\\re.py:212: FutureWarning: split() requires a non-empty pattern match.\n",
      "  return _compile(pattern, flags).split(string, maxsplit)\n"
     ]
    }
   ],
   "source": [
    "spamTest()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
